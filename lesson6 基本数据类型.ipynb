{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2684, -0.8986,  1.1558],\n",
       "        [ 0.2176, -0.6036, -0.2610]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type chack\n",
    "import torch\n",
    "a=torch.randn(2,3) #二维的tensor张量，随机正态分布 N（0，1）\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.type() #返回基本数据类型的详细信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a) #返回基本数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(a,torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(a,torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3407de775ab8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#将数据搬运到gpu上 返回索引\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "a=a.cuda() #将数据搬运到gpu上 返回索引\n",
    "isinstance(a,torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor torch.FloatTensor torch.LongTensor\n",
      "tensor(1.) tensor(1.3000) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "#张量 标量 维度之间的表示与认识\n",
    "import torch\n",
    "a=torch.tensor(1.) #torch.FloatTensor\n",
    "b=torch.tensor(1.3) #orch.FloatTensor\n",
    "c=torch.tensor(1) # torch.LongTensor\n",
    "print(a.type(),b.type(),c.type())\n",
    "print(a,b,c)#dim=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2000)\n",
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dim=0\n",
    "import torch\n",
    "a=torch.tensor(2.2)\n",
    "print(a)\n",
    "print(a.shape)  # 显示全局维度信息 \n",
    "print(a.size()) # 显示该维度的长度\n",
    "len(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1000])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "1\n",
      "\n",
      "\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dim=1\n",
    "import torch\n",
    "a=torch.tensor([1.1])\n",
    "print(a)\n",
    "print(a.shape)  # 显示全局维度信息 \n",
    "print(a.size()) # 显示该维度的长度\n",
    "print(len(a.shape))\n",
    "print(\"\\n\")\n",
    "b=torch.tensor([1.1,2.2,3.3])\n",
    "print(b.shape)  # 显示全局维度信息 数字表示该维度下向量的长度 ,几个数字表示几维数据\n",
    "print(b.size()) # 参数不指定则显示全局维度信息\n",
    "print(len(b.shape))\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.1633e+21])\n",
      "tensor([-4.1633e+21])\n",
      "tensor([1.0000, 1.0000])\n",
      "\n",
      "\n",
      "tensor([1., 1.])\n",
      "torch.Size([2])\n",
      "\n",
      "\n",
      "[1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dim=1 FloatTensor()应用 ones应用 list迁移到tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "a=torch.Tensor(1)\n",
    "b=torch.FloatTensor(1) #生成维度为1，向量长度为1的 tensor\n",
    "c=torch.FloatTensor(2) #生成维度为1，向量长度为2的 tensor\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(\"\\n\")\n",
    "\n",
    "d=torch.ones(2)\n",
    "print(d)\n",
    "print(d.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "data=np.ones(2) #生成维度为1，list数据每个元素都是1的串|数组\n",
    "print(data)\n",
    "e=torch.from_numpy(data) #将numpy数据转换成tensor默认数据类型\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7577,  2.2654,  0.5147],\n",
      "        [-0.4758,  2.5119,  0.9388]]) torch.Size([2, 3]) torch.Size([2, 3])\n",
      "2\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#dim=2\n",
    "import torch\n",
    "a=torch.randn(2,3)\n",
    "print(a,a.shape,a.size()) #torch.Size([2, 3]) 两个数字表示二维数据，每一维的长度分别是2和3\n",
    "\n",
    "print(a.size(0)) #0维度上的向量长度是2\n",
    "print(a.size(1)) #1维度上的向量长度是3\n",
    "\n",
    "print(a.shape[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2865, 0.2976, 0.0274, 0.0152],\n",
      "         [0.3216, 0.6000, 0.2234, 0.0638]],\n",
      "\n",
      "        [[0.1820, 0.6870, 0.7096, 0.8861],\n",
      "         [0.8449, 0.7022, 0.2568, 0.6619]],\n",
      "\n",
      "        [[0.1212, 0.9075, 0.9556, 0.0444],\n",
      "         [0.6315, 0.3709, 0.5806, 0.5285]]])\n",
      "torch.Size([3, 2, 4])\n",
      "tensor([[0.2865, 0.2976, 0.0274, 0.0152],\n",
      "        [0.3216, 0.6000, 0.2234, 0.0638]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 2, 4]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim=3\n",
    "import torch\n",
    "a=torch.rand(3,2,4)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a[0])\n",
    "list(a.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6843, 0.2932, 0.7865, 0.4036],\n",
      "         [0.3674, 0.6459, 0.7272, 0.0952]],\n",
      "\n",
      "        [[0.1063, 0.3525, 0.5847, 0.7114],\n",
      "         [0.3845, 0.4054, 0.6750, 0.3152]],\n",
      "\n",
      "        [[0.9955, 0.0046, 0.6979, 0.8180],\n",
      "         [0.9545, 0.5063, 0.3121, 0.7808]]]) \n",
      "\n",
      "tensor([[[[0.8392, 0.0568, 0.7579, 0.3329],\n",
      "          [0.7765, 0.1720, 0.1017, 0.5132]],\n",
      "\n",
      "         [[0.9378, 0.5783, 0.7810, 0.0074],\n",
      "          [0.3458, 0.4591, 0.6081, 0.2416]],\n",
      "\n",
      "         [[0.7068, 0.1938, 0.6932, 0.1228],\n",
      "          [0.3639, 0.5531, 0.3296, 0.7285]]],\n",
      "\n",
      "\n",
      "        [[[0.5022, 0.5182, 0.4624, 0.9228],\n",
      "          [0.8436, 0.1003, 0.1673, 0.8095]],\n",
      "\n",
      "         [[0.2983, 0.3930, 0.4544, 0.1026],\n",
      "          [0.8621, 0.8905, 0.1019, 0.4213]],\n",
      "\n",
      "         [[0.9177, 0.0451, 0.6906, 0.5293],\n",
      "          [0.7536, 0.3593, 0.6521, 0.4029]]]]) \n",
      "\n",
      " torch.Size([2, 3, 2, 4])\n",
      "24 48\n",
      "3 4 0 1\n"
     ]
    }
   ],
   "source": [
    "#dim=3 vs dim=4  numle函数（） dim（）\n",
    "a=torch.rand(3,2,4)\n",
    "print(a,\"\\n\")\n",
    "b=torch.rand(2,3,2,4)\n",
    "print(b,\"\\n\\n\",b.shape)\n",
    "\n",
    "c=torch.tensor(1)\n",
    "\n",
    "d=torch.tensor([1])\n",
    "\n",
    "print(a.numel(),b.numel())\n",
    "print(a.dim(),b.dim(),c.dim(),d.dim())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
